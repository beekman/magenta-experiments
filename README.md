A JavaScript-based audio experiment using Magenta.js to generate dynamic music performances, using user input to generate dynamic musical phrases and beats with the help of two different machine learning models for music and rhythm.

Part 2
------

Melody generation

* [Magenta.js Music](https://github.com/magenta/magenta-js/tree/master/music)
* [MusicRNN API docs](https://magenta.github.io/magenta-js/music/classes/_music_rnn_model_.musicrnn.html)
* [Pre-Trained Checkpoints for Magenta.js](https://github.com/magenta/magenta-js/blob/master/music/checkpoints/README.md#table)
* [Tone.Frequency for converting between MIDI and note names](https://tonejs.github.io/docs/14.7.39/fn/Frequency)
* On Magenta Note Sequences, see [tutorial](https://hello-magenta.glitch.me/#step1) and [utility functions](https://magenta.github.io/magenta-js/music/modules/_core_sequences_.html)

Drum grooving

* [The Drum MIDI mappings used by Magenta](https://github.com/magenta/magenta-js/blob/master/music/src/core/data.ts#L36)
* [MusicVAE API docs](https://magenta.github.io/magenta-js/music/classes/_music_vae_model_.musicvae.html)
* [GrooVAE](https://magenta.tensorflow.org/groovae)
* [Tone.Time API docs](https://tonejs.github.io/docs/14.7.39/fn/Time)

